{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN of Copy of AA on NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhyX3b91txQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from collections import deque\n",
        "from torch.distributions import Categorical\n",
        "import math as m\n",
        "from torch.nn.utils.convert_parameters import vector_to_parameters\n",
        "from IPython.display import clear_output\n",
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces.box import Box\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPAy6v9uw3V",
        "colab_type": "text"
      },
      "source": [
        "**Neural Nets**\n",
        "\n",
        "A3C, TRPO, DQN\n",
        "\n",
        "Neural Net Architecture are as described in the paper (Playing Atari with       Deep Reinforcement Learning), where\n",
        "The input to the neural network consits of 84 * 84 * 4 image\n",
        "The first hidden layer convolves 16 8x8 filters with stride 4\n",
        "The second hidden layer convolves 32 4x4 filters with stride 2\n",
        "The final hidden layer is fully connected and consists of 256 units\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l02wYDeJuD8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A3CLSTMNet(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, num_actions):\n",
        "        \n",
        "        super(A3CLSTMNet, self).__init__()\n",
        "        \n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 8, 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 4, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.lstm = nn.LSTMCell(32 * 9 * 9, 256)\n",
        "        \n",
        "        self.critic_linear = nn.Linear(256, 1)\n",
        "        self.actor_linear = nn.Linear(256, num_actions)\n",
        "        \n",
        "        self.name = 'a3c_lstm'\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x, (hx, cx) = inputs\n",
        "        x = torch.FloatTensor(x).view(-1, 1, 84, 84).to(device)\n",
        "        x = self.conv_layers(x)\n",
        "        \n",
        "        x = x.view(-1, 32 * 9 * 9)\n",
        "        hx, cx = self.lstm(x, (hx, cx))\n",
        "        x = hx\n",
        "        value = self.critic_linear(x)\n",
        "        return F.softmax(self.actor_linear(x), dim=1), (hx, cx)\n",
        "\n",
        "    def act(self, inputs):\n",
        "        prob, (hx, cx) = self.forward(inputs)\n",
        "        categorical = Categorical(prob)\n",
        "        action = categorical.sample()\n",
        "        return action, prob, (hx, cx)\n",
        "\n",
        "class TrpoNet(nn.Module):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "\n",
        "    super(TrpoNet, self).__init__()\n",
        "    self.conv_layers = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, 8, 4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, 4, 2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Linear(2592, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_actions)\n",
        "    )\n",
        "\n",
        "    self.name = 'trpo'\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = torch.FloatTensor(x).view(-1, 1, 84, 84).to(device)\n",
        "      x = self.conv_layers(x)\n",
        "      x = self.fc_layers(x.view(-1, 2592))\n",
        "      output = F.softmax(x,dim=1)\n",
        "      # Avoid one of the elements equal to 0\n",
        "      output = output + 1e-6\n",
        "      output = F.normalize(output, dim=1, p=1)\n",
        "      return output\n",
        "\n",
        "  def act(self, input):\n",
        "      prob = self.forward(input)\n",
        "      categorical = Categorical(prob)\n",
        "      action = categorical.sample()\n",
        "      return action, prob\n",
        "\n",
        "class DqnNet(nn.Module):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "    super(DqnNet, self).__init__()\n",
        "    self.conv_layers = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, 8, 4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, 4, 2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Linear(2592, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_actions)\n",
        "    )\n",
        "\n",
        "    self.name = 'dqn'\n",
        "\n",
        "  def forward(self, x):\n",
        "      # Convert input to an tensor with size (n, 1, 84, 84)\n",
        "      x = torch.FloatTensor(x).view(-1, 1, 84, 84).to(device)\n",
        "      x = self.conv_layers(x)\n",
        "      x = self.fc_layers(x.view(-1, 2592))\n",
        "      return x\n",
        "  \n",
        "  def act(self, input):\n",
        "      q_values = self.forward(input)\n",
        "      action = q_values.max(1)[1]\n",
        "      # Get softmax of q_values as said in paper\n",
        "      prob = F.softmax(q_values, dim=1)\n",
        "      return action, prob"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkGopzDqzqww",
        "colab_type": "text"
      },
      "source": [
        "**Environment**\n",
        "\n",
        "84x84 gray image\n",
        "\n",
        "Taken from https://github.com/nailo2c/a3c/blob/master/envs.py and deepmind wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhirhIWFzomi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_atari_env(env_id):\n",
        "    env = gym.make(env_id)\n",
        "    env = AtariRescale84x84(env)\n",
        "    env = NormalizedEnv(env)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_frame84(frame):\n",
        "    frame = frame[34:34 + 160, :160]\n",
        "    frame = cv2.resize(frame, (84, 84))\n",
        "    frame = frame.mean(2)\n",
        "    frame = frame.astype(np.float32)\n",
        "    frame *= (1.0 / 255.0)\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "  \n",
        "\n",
        "class AtariRescale84x84(gym.ObservationWrapper):\n",
        "\n",
        "    def __init__(self, env=None):\n",
        "        super(AtariRescale84x84, self).__init__(env)\n",
        "        self.observation_space = Box(0.0, 1.0, [1, 84, 84])\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return process_frame84(observation) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NormalizedEnv(gym.ObservationWrapper):\n",
        "\n",
        "    def __init__(self, env=None):\n",
        "        super(NormalizedEnv, self).__init__(env)\n",
        "        self.state_mean = 0\n",
        "        self.state_std = 0\n",
        "        self.alpha = 0.9999\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.num_steps += 1\n",
        "        self.state_mean = self.state_mean * self.alpha + \\\n",
        "            observation.mean() * (1 - self.alpha)\n",
        "        self.state_std = self.state_std * self.alpha + \\\n",
        "            observation.std() * (1 - self.alpha)\n",
        "\n",
        "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
        "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
        "        ret = (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
        "        return np.expand_dims(ret, axis=0)\n",
        "\n",
        "\n",
        "class NormalizedState:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state_mean = 0\n",
        "        self.state_std = 0\n",
        "        self.alpha = 0.9999\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.num_steps += 1\n",
        "        self.state_mean = self.state_mean * self.alpha + \\\n",
        "            observation.mean() * (1 - self.alpha)\n",
        "        self.state_std = self.state_std * self.alpha + \\\n",
        "            observation.std() * (1 - self.alpha)\n",
        "\n",
        "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
        "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
        "        ret = (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
        "        return np.expand_dims(ret, axis=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlemVhJ00ILB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_id = 'PongNoFrameskip-v4'\n",
        "env = create_atari_env(env_id)\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whqt_SZvurLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn_model = DqnNet(num_actions)\n",
        "dqn_model2 = DqnNet(num_actions)\n",
        "trpo_model = TrpoNet(num_actions)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uomv-PH24RH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f744caee-cc65-4a74-e5f9-ffab615e540d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErxaQw774NeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "385f29f0-052b-4487-db5d-c2fcc9d7649c"
      },
      "source": [
        "ac3_path = '/content/gdrive/My Drive/Summer Research/AA on NN/models1/A3C_PongNoFrameskip_v4'\n",
        "trpo_path = '/content/gdrive/My Drive/Summer Research/AA on NN/models2/TRPO_PongNoFrameSkip_V4_skip4'\n",
        "dqn_path = '/content/gdrive/My Drive/Summer Research/AA on NN/models2/DQN_PongNoFrameSkip_V4_skip4'\n",
        "dqn_path2 = '/content/gdrive/My Drive/Summer Research/AA on NN/models1/DQN_PongNoFrameskip_v4'\n",
        "trpo_model.load_state_dict(torch.load(trpo_path))\n",
        "dqn_model.load_state_dict(torch.load(dqn_path))\n",
        "dqn_model2.load_state_dict(torch.load(dqn_path2))\n",
        "dqn_model2.cuda()\n",
        "trpo_model.cuda()\n",
        "dqn_model.cuda()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DqnNet(\n",
              "  (conv_layers): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(8, 8), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (fc_layers): Sequential(\n",
              "    (0): Linear(in_features=2592, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-TN46FA_I1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_perturbation(state, selected_action, y_dist_weight, eps=0.1, type='l1'):\n",
        "  # Get the cross entropy loss between the weight of y and selected action\n",
        "  y_dist_weight = y_dist_weight.view(1, -1)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  loss = criterion(y_dist_weight, selected_action)\n",
        "  grad = torch.autograd.grad(loss, state)[0]\n",
        "  if(type=='l_inf'):\n",
        "    #new_state = state + eps * (state.max() - state.min())*  torch.sign(grad)\n",
        "    perturbation = eps *  torch.sign(grad)\n",
        "  if(type=='l2'):\n",
        "    perturbation = eps * math.sqrt(84 * 84) *  (grad / torch.sqrt(grad.pow(2).sum()))\n",
        "  if(type == 'l1'):\n",
        "    budget = eps * 84 * 84\n",
        "    perturbation = grad.pow(10)\n",
        "    perturbation = perturbation / perturbation.sum() * budget\n",
        "  if(eps == 0):\n",
        "    perturbation = 0\n",
        "\n",
        "  return perturbation"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si5lYZLc56gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_black_box(model, env, type):\n",
        "    rewards = []\n",
        "    episodes = 10\n",
        "    for eps in range(0, 9):\n",
        "        avg_episode_reward = 0\n",
        "        print('eps:', eps * 0.3)\n",
        "        for episode in range(episodes):\n",
        "          state = env.reset()\n",
        "          eps_reward = 0\n",
        "          if(model.name == 'a3c_lstm'):\n",
        "            cx = torch.zeros(1, 256).to(device)\n",
        "            hx = torch.zeros(1, 256).to(device)\n",
        "          while(True):\n",
        "            if(model.name == 'a3c_lstm'):\n",
        "              #state = normalizer_1.observation(state)\n",
        "              state = torch.tensor(state, requires_grad=True)\n",
        "              action, prob, _ = model.act((state, (hx, cx)))\n",
        "              perturbated_state = apply_perturbation(state, action, prob, eps * 0.3, type)\n",
        "              #perturbated_state = normalizer_2.observation(perturbated_state.detach().numpy())\n",
        "              action, _, (hx, cx) = model.act((perturbated_state, (hx, cx)))\n",
        "            else:\n",
        "              #state = normalizer_1.observation(state)\n",
        "              state = torch.tensor(state, requires_grad=True)\n",
        "              action, prob = model.act(state)\n",
        "              #print(torch.autograd.grad(prob.mean(), state))\n",
        "              perturbated_state = apply_perturbation(state, action, prob, eps * 0.3, type)\n",
        "              #perturbated_state = normalizer_2.observation(perturbated_state.detach().numpy())\n",
        "              action, _ = model.act(perturbated_state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            eps_reward += reward\n",
        "            state = next_state\n",
        "            if(done):\n",
        "              state = env.reset()\n",
        "              avg_episode_reward += eps_reward\n",
        "              print(eps_reward)\n",
        "              break\n",
        "        avg_episode_reward /= episodes\n",
        "        rewards.append(avg_episode_reward)\n",
        "\n",
        "    return rewards"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvv6ws15hqMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_white_box(model, adversarial_model, env, type):\n",
        "    rewards = []\n",
        "    episodes = 10\n",
        "    for eps in range(0, 9):\n",
        "        avg_episode_reward = 0\n",
        "        print('eps:', eps * 0.3)\n",
        "        for episode in range(episodes):\n",
        "          state = env.reset()\n",
        "          eps_reward = 0\n",
        "          if(adversarial_model.name == 'a3c_lstm'):\n",
        "            acx = torch.zeros(1, 256).to(device)\n",
        "            ahx = torch.zeros(1, 256).to(device)\n",
        "          if(model.name == 'a3c_lstm'):\n",
        "            cx = torch.zeros(1, 256).to(device)\n",
        "            hx = torch.zeros(1, 256).to(device)\n",
        "          while(True):\n",
        "            if(adversarial_model.name == 'a3c_lstm'):\n",
        "              state = torch.tensor(state, requires_grad=True)\n",
        "              action, prob, (acx, ahx) = adversarial_model.act((state, (acx, ahx)))\n",
        "              perturbated_state = calculate_perturbation(state, action, prob, eps * 0.3, type) + state\n",
        "            else:\n",
        "              state = torch.tensor(state, requires_grad=True)\n",
        "              action, prob = adversarial_model.act(state)\n",
        "              perturbated_state = calculate_perturbation(state, action, prob, eps * 0.3, type) + state\n",
        "            \n",
        "            if(model.name == 'a3c_lstm'):\n",
        "              action, _, (hx, cx) = model.act((perturbated_state, (hx, cx)))\n",
        "            else:\n",
        "              action, _ = model.act(perturbated_state)\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            eps_reward += reward\n",
        "            state = next_state\n",
        "            if(done):\n",
        "              state = env.reset()\n",
        "              avg_episode_reward += eps_reward\n",
        "              print(eps_reward)\n",
        "              break\n",
        "        avg_episode_reward /= episodes\n",
        "        rewards.append(avg_episode_reward)\n",
        "\n",
        "    return rewards"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onJ43P1_I0L-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards, epsilons = test_white_box(dqn_model, trpo_model, env, 'l1')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}