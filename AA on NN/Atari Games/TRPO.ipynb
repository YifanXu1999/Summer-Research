{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  TRPO Atari.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMmD4QSCurvY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXSCB79tutVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from collections import deque\n",
        "from torch.distributions import Categorical\n",
        "import math as m\n",
        "from torch.nn.utils.convert_parameters import vector_to_parameters\n",
        "from IPython.display import clear_output\n",
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces.box import Box\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdiU-oILut-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_atari_env(env_id):\n",
        "    env = gym.make(env_id)\n",
        "    env = AtariRescale84x84(env)\n",
        "    env = NormalizedEnv(env)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_frame84(frame):\n",
        "    frame = frame[34:34 + 160, :160]\n",
        "    frame = cv2.resize(frame, (84, 84))\n",
        "    frame = frame.mean(2)\n",
        "    frame = frame.astype(np.float32)\n",
        "    frame *= (1.0 / 255.0)\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "  \n",
        "\n",
        "class AtariRescale84x84(gym.ObservationWrapper):\n",
        "\n",
        "    def __init__(self, env=None):\n",
        "        super(AtariRescale84x84, self).__init__(env)\n",
        "        self.observation_space = Box(0.0, 1.0, [1, 84, 84])\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return process_frame84(observation) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NormalizedEnv(gym.ObservationWrapper):\n",
        "\n",
        "    def __init__(self, env=None):\n",
        "        super(NormalizedEnv, self).__init__(env)\n",
        "        self.state_mean = 0\n",
        "        self.state_std = 0\n",
        "        self.alpha = 0.9999\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.num_steps += 1\n",
        "        self.state_mean = self.state_mean * self.alpha + \\\n",
        "            observation.mean() * (1 - self.alpha)\n",
        "        self.state_std = self.state_std * self.alpha + \\\n",
        "            observation.std() * (1 - self.alpha)\n",
        "\n",
        "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
        "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
        "        ret = (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
        "        return np.expand_dims(ret, axis=0)\n",
        "\n",
        "\n",
        "class NormalizedState:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state_mean = 0\n",
        "        self.state_std = 0\n",
        "        self.alpha = 0.9999\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.num_steps += 1\n",
        "        self.state_mean = self.state_mean * self.alpha + \\\n",
        "            observation.mean() * (1 - self.alpha)\n",
        "        self.state_std = self.state_std * self.alpha + \\\n",
        "            observation.std() * (1 - self.alpha)\n",
        "\n",
        "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
        "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
        "        ret = (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
        "        return np.expand_dims(ret, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQbxU5XJpjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_id = 'PongNoFrameskip-v4'\n",
        "env = create_atari_env(env_id)\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqnEh22yu6Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cuda device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "gamma = 0.99\n",
        "res_threshold = 1e-10\n",
        "cg_max_iters = 10\n",
        "delta =  0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kry6qUxfvK5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrpoNet(nn.Module):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "\n",
        "    super(TrpoNet, self).__init__()\n",
        "    self.conv_layers = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, 8, 4),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, 4, 2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Linear(2592, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_actions)\n",
        "    )\n",
        "\n",
        "    self.name = 'trpo'\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = torch.FloatTensor(x).view(-1, 1, 84, 84).to(device)\n",
        "      x = self.conv_layers(x)\n",
        "      x = self.fc_layers(x.view(-1, 2592))\n",
        "      output = F.softmax(x,dim=1)\n",
        "      # Avoid one of the elements equal to 0\n",
        "      output = output + 1e-6\n",
        "      output = F.normalize(output, dim=1, p=1)\n",
        "      return output\n",
        "\n",
        "  def act(self, input):\n",
        "      prob = self.forward(input)\n",
        "      categorical = Categorical(prob)\n",
        "      action = categorical.sample()\n",
        "      return action, prob\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5ADW7va0aDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrajectoryRecord:\n",
        "  def __init__(self):\n",
        "    self.rewards = []\n",
        "    self.actions = []\n",
        "    self.states = []\n",
        "    self.dones = []\n",
        "  \n",
        "  def push(self, reward, action, state, done):\n",
        "    self.rewards.append(reward)\n",
        "    self.actions.append(action)\n",
        "    self.states.append(state)\n",
        "    self.dones.append(done)\n",
        "  \n",
        "  def returnRecord(self):\n",
        "    rewards = self.rewards\n",
        "    states = self.states\n",
        "    actions = self.actions\n",
        "    dones = self.dones\n",
        "    return rewards, states, actions, dones\n",
        "\n",
        "  def reset(self):\n",
        "    self.rewards = []\n",
        "    self.actions = []\n",
        "    self.states = []\n",
        "    self.dones = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfEuSx2_6HwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_surrogate_loss(selected_action_prbs, q_values):\n",
        "    '''\n",
        "    L = mean of (pi(a_n|s_n) / pi_old(a_n|sn) * Q_old(s_n, a_n)\n",
        "    '''\n",
        "    L = (selected_action_prbs / selected_action_prbs.data) * torch.FloatTensor(q_values).to(device)\n",
        "    return L.mean()\n",
        "\n",
        "def get_Q(rewards, dones):\n",
        "    '''\n",
        "    This method computes and returns the action-state values along the generated\n",
        "    trajectory\n",
        "    '''\n",
        "    R = 0\n",
        "    Q = []\n",
        "    for r, done in zip(reversed(rewards), reversed(dones)):\n",
        "        R = r + gamma * R * (1 - done) \n",
        "        Q.insert(0, R)\n",
        "    return Q\n",
        "def flat_parameters(param):\n",
        "    '''\n",
        "    Convert a list of tensors with different sizes into an 1d array of parameters\n",
        "    '''\n",
        "    return torch.cat([grad.contiguous().view(-1) for grad in param])\n",
        "def get_fisher_vector_product(x, prbs, model, damping = 1e-2):\n",
        "    '''\n",
        "    FVP is used to indirectly compute hassin matrix with more efficency, and it\n",
        "    is used for conjugate gradient.\n",
        "    y = Hx\n",
        "    '''\n",
        "    # Step 1, compute the product of first derivative of KL divergence wrt theta and x\n",
        "    kl = get_kl(prbs)\n",
        "    model.zero_grad()\n",
        "    kl_1_grads_ = torch.autograd.grad(kl, model.parameters(), create_graph = True, retain_graph = True)\n",
        "    kl_1_grads = flat_parameters(kl_1_grads_)\n",
        "    # Step2, compute the sum of the product of kl first derivative and x\n",
        "    kl_1_grads_product = kl_1_grads * x\n",
        "    kl_1_grads_product_sum = kl_1_grads_product.sum()\n",
        "    # Step3, obtain fisher_vector_product by differentiating the result we get at step2\n",
        "    model.zero_grad()\n",
        "    kl_2_grads = torch.autograd.grad(kl_1_grads_product_sum, model.parameters(), retain_graph = True)\n",
        "    fisher_vector_product = flat_parameters(kl_2_grads)\n",
        "    return fisher_vector_product + damping * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3D4XY1l7Xi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_kl_compare(pi, pi_old):\n",
        "    return (pi_old * torch.log(pi_old / pi)).mean().item()\n",
        "\n",
        "def get_kl(pi):\n",
        "    '''\n",
        "    input: state\n",
        "    This method computes the KL divergence given the input state, where\n",
        "    kl(pi, pi_old) = mean of (pi_old * log (pi_old/pi), where pi = pi_old,\n",
        "    grad of pi should be enabled, and grad of pi_old should be disabled\n",
        "    '''\n",
        "    pi_old = pi.data\n",
        "    result = (pi_old * torch.log((pi_old / pi))).mean()\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq2-iEko-chp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conjugate_gradient(b, prbs, model):\n",
        "    '''\n",
        "    Algorithm from wiki\n",
        "    ---------------------------------------------------------------------------\n",
        "    function x = conjgrad(A, b, x)\n",
        "        r = b - A * x;\n",
        "        p = r;\n",
        "        rsold = r' * r;\n",
        "    \n",
        "        for i = 1:length(b)\n",
        "            Ap = A * p;\n",
        "            alpha = rsold / (p' * Ap);\n",
        "            x = x + alpha * p;\n",
        "            r = r - alpha * Ap;\n",
        "            rsnew = r' * r;\n",
        "            if sqrt(rsnew) < 1e-10\n",
        "                  break;\n",
        "            end\n",
        "            p = r + (rsnew / rsold) * p;\n",
        "            rsold = rsnew;\n",
        "        end\n",
        "    ---------------------------------------------------------------------------\n",
        "    end\n",
        "    '''\n",
        "    # Init a x\n",
        "    x = torch.zeros(b.size()).to(device)\n",
        "    # b - A * x = b because x = 0\n",
        "    r = b.clone()\n",
        "    p = r.clone()\n",
        "    rsold = r.dot(r)\n",
        "    for i in range(cg_max_iters):\n",
        "        # A  = get_fisher_vector_product()\n",
        "        Ap = get_fisher_vector_product(p, prbs, model)\n",
        "        alpha = rsold / (p.dot(Ap))\n",
        "        x = x + alpha * p\n",
        "        r = r - alpha * Ap\n",
        "        rsnew = r.dot(r)\n",
        "        if m.sqrt(rsnew) < res_threshold:\n",
        "            break\n",
        "        p = r + (rsnew / rsold) * p\n",
        "        rsold = rsnew;\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkSOcgmCF5eI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_theta(theta, beta, s, model, old_prbs, states):\n",
        "    '''\n",
        "    This function computes and updates an appropriate theta, such that Dkl(pi, pi_old) < delta\n",
        "    If with the current beta, the constraint doesnt hold, decresease the beta value exponentially\n",
        "    '''\n",
        "    beta_factor = 1\n",
        "    beta_s = beta * s\n",
        "    before = 0\n",
        "    revert_theta = theta.clone()\n",
        "    print('theta max', theta.max(), 'theta isNan', torch.isnan(theta).any())\n",
        "    with torch.no_grad():\n",
        "        for i in range(10):\n",
        "            new_theta = theta + torch.clamp(beta_factor * beta_s, min=-40, max=40)\n",
        "            print('new_theta max', new_theta.max(), 'new_theta isNan', torch.isnan(new_theta).any())\n",
        "            #print(beta_factor * beta_s)\n",
        "            vector_to_parameters(new_theta, model.parameters())\n",
        "            beta_factor = beta_factor / m.e\n",
        "            new_prbs = model(states)\n",
        "            print('kl divergence', i, get_kl_compare(new_prbs, old_prbs))\n",
        "            if(m.isnan(get_kl_compare(new_prbs, old_prbs))):\n",
        "                print('revert theta')\n",
        "                vector_to_parameters(revert_theta, model.parameters())\n",
        "                break\n",
        "            if(get_kl_compare(new_prbs, old_prbs) <= 2 * delta):\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxgH9JVdBLRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(record, model):\n",
        "  rewards, states, actions, dones = record.returnRecord()\n",
        "  prbs = model(states)\n",
        "  actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
        "  selected_action_prbs = prbs.gather(1, actions).squeeze(1)\n",
        "  q_values = get_Q(rewards, dones)\n",
        "\n",
        "  L = get_surrogate_loss(selected_action_prbs, q_values)\n",
        "  model.zero_grad()\n",
        "  g_ = torch.autograd.grad(L, model.parameters(), retain_graph=True)\n",
        "  g = flat_parameters(g_)\n",
        "  s = conjugate_gradient(g, prbs, model)\n",
        "  Hs = get_fisher_vector_product(s, prbs, model, damping=0)\n",
        "  sHs = s.dot(Hs)\n",
        "\n",
        "  beta =  m.sqrt(2*delta / sHs)\n",
        "  theta = flat_parameters(model.parameters())\n",
        "\n",
        "  update_theta(theta, beta, s, model, prbs, states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDmgzr4HehgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_select_action(state, model):\n",
        "    prbs = model(state)\n",
        "    categorical = Categorical(prbs)\n",
        "    action = categorical.sample()\n",
        "    return int(action), prbs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRh95EA5JMfp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8nqjWL2IYbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(model):\n",
        "    policy_iter_rewards = []\n",
        "    record = TrajectoryRecord()\n",
        "    last_policy_avg_rewards = 0\n",
        "    for policy_iter in range(5000):\n",
        "        record.reset()\n",
        "        state = env.reset()\n",
        "        epoch_reward = 0\n",
        "        policy_total_rewards = 0\n",
        "        epochs = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "        while (step < 1000 or not done):\n",
        "            action, prb = policy_select_action(state, model)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            record.push(reward, action, state, done)\n",
        "            epoch_reward += reward\n",
        "            state = next_state\n",
        "            step+=1\n",
        "            if(done):\n",
        "              epochs += 1\n",
        "              policy_total_rewards += epoch_reward\n",
        "              epoch_reward = 0\n",
        "              state = env.reset()\n",
        "        last_policy_avg_rewards = policy_total_rewards / epochs\n",
        "        update_policy(record, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nF1kFrCq9_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TrpoNet(env.action_space.n).cuda()\n",
        "\n",
        "main(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}